{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie polarity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/fastai/')\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/ubuntu/fastai/projects/part_1_material/RNNs/data/movie_review_2class/'\n",
    "trn_path = 'trn/all'\n",
    "val_path = 'val/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_files = !ls {PATH}{trn_path}/pos\n",
    "trin_neg_files = !ls {trn_path}/neg\n",
    "val_pos_files = !ls {val_path}/pos\n",
    "val_neg_files = !ls {val_path}/neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's look at a positive movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-603bb93e0890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_review\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_review' is not defined"
     ]
    }
   ],
   "source": [
    "' '.join([part for part in test_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = dict(train=trn_path, validation=val_path, test=val_path)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the `TEXT` object that we had instansiated above has the properties of \n",
    "`TEXT.vocab.itos` (int-to-string) which is the vocabulary for the corpus (i.e. all the words) as well as the mapping dictionary\n",
    "from word to token via `TEXT.vocab.stoi` (string-to-int). We can see that these provide methods for mapping between tokens and strings, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', ',', 'the', '.', 'a', 'and', 'of', 'to', 'is', 'in', \"'s\"]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:12])\n",
    "print(TEXT.vocab.stoi['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do now is to dump the tokenized text data to a `.pickle` file so that we can read this later instead of having to do the tokenization again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some information about the movie reviews such as the # batches in the data set, # of tokens in the vocab, # of tokens in the training set, and the # of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 277\n",
      "Number of tokens in vocab: 8162\n",
      "Number of tokens in training set: 1\n",
      "Number of sentences: 1245853\n"
     ]
    }
   ],
   "source": [
    "print('Number of batches: {}'.format(len(md.trn_dl)))\n",
    "print('Number of tokens in vocab: {}'.format(md.nt))\n",
    "print('Number of tokens in training set: {}'.format(len(md.trn_ds)))\n",
    "print('Number of sentences: {}'.format(len(md.trn_ds[0].text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note about the LanguageModelData object: 1) There is only one item in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'jet',\n",
       " 'li',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chinese',\n",
       " 'cop',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'help',\n",
       " 'some']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0].text[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The LanguageModelData object will create batches with 64 columns (the batch size), and varying sequence lengths around 70 (the bptt we set above). Furthermore, the batches will contain the exact same number of data as labels, but offset by one, because we are trying to predict the next word in the sequence. The labels are a 1D flattened array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "   114    26  7663  ...   1910  2647    24\n",
       "    58    14   139  ...      4  1390  2649\n",
       "  2700     9    96  ...    398    10   721\n",
       "        ...          ⋱          ...       \n",
       "     4     8     9  ...    255     6     0\n",
       "     4    30  3609  ...     43  1692   589\n",
       "     4    33   151  ...     28   163   152\n",
       " [torch.cuda.LongTensor of size 68x64 (GPU 0)], Variable containing:\n",
       "    58\n",
       "    14\n",
       "   139\n",
       "   ⋮  \n",
       "    31\n",
       "  1012\n",
       "   116\n",
       " [torch.cuda.LongTensor of size 4352 (GPU 0)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the other parameters needed for fast.ai LanguageModels, mainly the embedding dimensions, the number of hidden units per layer,  and the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding size\n",
    "emb_sz = 200\n",
    "# Number of hidden units\n",
    "nh = 250\n",
    "# Number of hidden layers\n",
    "nl = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define the optimizer we want to use, as a default use Adam with the momentum set to less than the default of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn=opt, emb_sz=emb_sz, n_hid=nh,\n",
    "                       n_layers=nl, dropouti=0.05, dropout=0.05,\n",
    "                       wdrop=0.1, dropoute=0.02, dropouth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a regularization function for this type of learner as well as a clipping percentage for gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab6108069542998a1fdfbde78e2370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      5.54098    5.337223  \n",
      "    1      5.101896   4.975279                              \n",
      "    2      4.888684   4.800024                              \n",
      "    3      4.751794   4.698601                              \n",
      "    4      4.654961   4.635167                              \n",
      "    5      4.584552   4.591514                              \n",
      "    6      4.522377   4.564551                              \n",
      "    7      4.481195   4.550163                              \n",
      "    8      4.48174    4.54351                               \n",
      "    9      4.465264   4.541524                              \n",
      "    10     4.540777   4.546882                              \n",
      "    11     4.467847   4.51085                               \n",
      "    12     4.404246   4.483813                              \n",
      "    13     4.338234   4.466834                              \n",
      "    14     4.280584   4.449007                              \n",
      "    15     4.234407   4.437104                              \n",
      "    16     4.207338   4.430138                              \n",
      "    17     4.20937    4.423335                              \n",
      "    18     4.15579    4.424022                              \n",
      "    19     4.140133   4.425146                              \n",
      "    20     4.305556   4.444969                              \n",
      "    21     4.258089   4.436969                              \n",
      "    22     4.199954   4.426712                              \n",
      "    23     4.158779   4.424387                              \n",
      "    24     4.111577   4.418464                              \n",
      "    25     4.07718    4.415823                              \n",
      "    26     4.053699   4.410531                              \n",
      "    27     4.035813   4.410834                              \n",
      "    28     3.995604   4.411452                              \n",
      "    29     4.002933   4.41508                               \n",
      "    30     4.156119   4.423356                              \n",
      "    31     4.124965   4.426664                              \n",
      "    32     4.080239   4.422017                              \n",
      "    33     4.052619   4.423243                              \n",
      "    34     4.008302   4.424974                              \n",
      "    35     3.986601   4.412922                              \n",
      "    36     3.934031   4.420013                              \n",
      "    37     3.926691   4.418504                              \n",
      "    38     3.900782   4.423085                              \n",
      "    39     3.918036   4.427202                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.4272])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 4, wds=1e-6, cycle_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used here is a seq2seq model so we want to save the first half of the model which is the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam1_enc')\n",
    "# learner.load_encoder('adam1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea45810f6a9431687e2a1f477a872a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      4.006791   4.440831  \n",
      "    1      4.018357   4.436441                              \n",
      "    2      4.033728   4.415796                              \n",
      "    3      3.96038    4.432343                              \n",
      "    4      3.925398   4.433996                              \n",
      "    5      3.889835   4.435346                              \n",
      "    6      3.895788   4.420619                              \n",
      "    7      3.898626   4.425046                              \n",
      "    8      3.817986   4.433572                              \n",
      "    9      3.812217   4.440577                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.44058])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('adam3_10_enc')\n",
    "#learner.load_encoder('adam3_10_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models are usually measured in terms of perplexity, which is simply exp() of our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.77494167382804"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the trained language model to do our sentiment analysis. First we will need the vocab, which we can load from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviePolarityDataset(torchtext.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        \n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for label in ['pos', 'neg']:\n",
    "            fnames = glob(os.path.join(path, label, '*txt'))\n",
    "            assert fnames, f\"can't find 'pos.txt' or 'neg.txt' under {path}{label}\"\n",
    "            for fname in fnames:\n",
    "                with open(fname, 'r') as f:\n",
    "                    text = f.readline()\n",
    "                    examples.append(data.Example.fromlist([text, label], fields))\n",
    "        \n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sortkey(ex): \n",
    "        return len(ex.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root=\".data\", train='trn', test='val', **kwargs):\n",
    "        return super().splits(\n",
    "            root, text_field=text_field, label_field=label_field, train=train,\n",
    "            validation=None, test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_POL_LABEL = data.Field(sequential=False)\n",
    "#splits = torchtext.datasets.IMDB.splits(TEXT, MOVIE_POL_LABEL, 'data/')\n",
    "spliter = MoviePolarityDataset('{}trn/'.format(PATH), TEXT, MOVIE_POL_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = splits[0].examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 'people claim its edited funny but they had to cut it down')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.label, ' '.join(t.text[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "md2 = TextData.from_splits(PATH, splits, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = md2.get_model(opt, 1500, bptt, emb_sz=emb_sz, n_hid=nh, n_layers=nl, \n",
    "           dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3)\n",
    "m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "m3.load_encoder(f'adam3_10_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.clip=25.\n",
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cf61d935de43379f86ae6649f07cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.717019   0.687749   0.622892  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac0213191bf44daa27d419f1fdfac74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.684837   0.65899    0.627544  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.65899]), 0.6275443832001123]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.freeze_to(-1)\n",
    "m3.fit(lrs/2, 1, metrics=[accuracy])\n",
    "m3.unfreeze()\n",
    "m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf57df2ac2954511985d79fc50b242be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                    \n",
      "    0      0.681997   0.63983    0.662952  \n",
      "    1      0.644367   0.627821   0.671777                    \n",
      "    2      0.646153   0.61158    0.699472                    \n",
      "    3      0.619255   0.619277   0.705013                    \n",
      "    4      0.620732   0.628516   0.725349                    \n",
      "    5      0.597886   0.622175   0.726817                    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.62217]), 0.7268174965217065]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(lrs, 3, metrics=[accuracy], cycle_len=2, cycle_save_name='movie_polarity_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
